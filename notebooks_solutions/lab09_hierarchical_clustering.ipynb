{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join('..', 'data', 'data_preprocessed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature names into groups\n",
    "# Remember which metric_features we decided to keep?\n",
    "metric_features = ['income',\n",
    " 'frq',\n",
    " 'rcn',\n",
    " 'clothes',\n",
    " 'kitchen',\n",
    " 'small_appliances',\n",
    " 'toys',\n",
    " 'house_keeping',\n",
    " 'per_net_purchase',\n",
    " 'spent_online']\n",
    "\n",
    "non_metric_features = df.columns[df.columns.str.startswith('oh_')].tolist() # CODE HERE\n",
    "pc_features = df.columns[df.columns.str.startswith('PC')].tolist()  # CODE HERE\n",
    "\n",
    "unused_features = [i for i in df.columns if i not in (metric_features+non_metric_features+pc_features) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('metric_features:', metric_features)\n",
    "print('non_metric_features:', non_metric_features)\n",
    "print('unused_features:', unused_features)\n",
    "print('pc_features:', pc_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "What is hierarchical clustering? \n",
    "\n",
    "How does it work? \n",
    "\n",
    "How does it relate to the distance matrix we discussed at the beginning of the course? ;)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](../figures/clustering/hierarch.gif)\n",
    "\n",
    "(From https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The distance matrix\n",
    "![](../figures/clustering/hc_distance_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of linkage\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_linkage_comparison_001.png)\n",
    "\n",
    "### How are they computed?\n",
    "![](../figures/clustering/linkage_types.jpeg)\n",
    "\n",
    "**Ward linkage**: minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics:\n",
    "- *bottom up approach*: each observation starts in its own cluster, and clusters are successively merged together\n",
    "- *greedy/local algorithm*: at each iteration tries to minimize the distance of cluster merging\n",
    "- *no realocation*: after an observation is assigned to a cluster, it can no longer change\n",
    "- *deterministic*: you always get the same answer when you run it\n",
    "- *scalability*: can become *very slow* for a large number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](../figures/clustering/hierarch.gif)\n",
    "\n",
    "(From https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics:\n",
    "- *bottom up approach*: each observation starts in its own cluster, and clusters are successively merged together\n",
    "- *greedy/local algorithm*: at each iteration tries to minimize the distance of cluster merging\n",
    "- *no realocation*: after an observation is assigned to a cluster, it can no longer change\n",
    "- *deterministic*: you always get the same answer when you run it\n",
    "- *scalability*: can become *very slow* for a large number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to apply Hierarchical Clustering?\n",
    "**Note: Which types of variables should be used for clustering?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing HC\n",
    "hclust = AgglomerativeClustering(linkage='ward', metric='euclidean', n_clusters=5)\n",
    "hc_labels = hclust.fit_predict(df[metric_features]) # CODE HERE\n",
    "hc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the clusters\n",
    "\n",
    "labels_series = pd.Series(hc_labels, \n",
    "                          name='labels', \n",
    "                          index=df.index # WHY df.index ??\n",
    "                          ) \n",
    "\n",
    "df_concat = pd.concat([df, labels_series],axis=1)\n",
    "\n",
    "df_concat[metric_features+['labels']].groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are we done?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "### ... Not yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the linkage method to choose:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to understand the following:**\n",
    "\n",
    "---\n",
    "\n",
    "$$SS_{t} = \\sum\\limits_{i = 1}^n {{{({x_i} - \\overline x )}^2}}$$\n",
    "\n",
    "$$SS_{w} = \\sum\\limits_{k = 1}^K {\\sum\\limits_{i = 1}^{{n_k}} {{{({x_i} - {{\\overline x }_k})}^2}} }$$\n",
    "\n",
    "$$SS_{b} = \\sum\\limits_{k = 1}^K {{n_k}{{({{\\overline x }_k} - \\overline x )}^2}}$$\n",
    "\n",
    "---\n",
    "\n",
    "$$SS_{t} = SS_{w} + SS_{b}$$\n",
    "\n",
    "---\n",
    "\n",
    "where \n",
    "\n",
    "$n$ is the total number of observations, \n",
    "\n",
    "$x_i$ is the vector of the $i^{th}$ observation, \n",
    "\n",
    "$\\overline x$ is the centroid of the data, \n",
    "\n",
    "$K$  is the number of clusters, \n",
    "\n",
    "$n_k$ is the number of observations in the $k^{th}$ cluster,\n",
    "\n",
    "and $\\overline x_k$ is the centroid of the $k^{th}$ cluster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figures/clustering/ss_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Calculate the `SS_` values using Pandas / NumPy:\n",
    "\n",
    "*Hint: Using numpy*\n",
    "\n",
    "$x_i$ : `X = data.values`\n",
    "\n",
    "$\\overline{x}$ : `X.mean()`\n",
    "\n",
    "$\\overline{x}_k$ : `X[hc_labels==k].mean()`\n",
    "\n",
    "$n_k$ : `X[hc_labels==k].shape[0]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMPY solution:\n",
    "\n",
    "# x_i       : X = df.values     : data\n",
    "# x_bar     : X.mean(axis=0)    : centroid of data\n",
    "# x_k       : X[hc_labels==k]   : points of cluster k\n",
    "# x_bar_k   : X_k.mean(axis=0)  : centroid of cluster k\n",
    "# n_k       : X_k.shape[0]\n",
    "\n",
    "X = df[metric_features].values\n",
    "\n",
    "# Computing SST\n",
    "sst = np.sum(np.square(X - X.mean(axis=0)), axis=0)\n",
    "\n",
    "# Computing SSW\n",
    "ssw_iter = []                   # Outer summation\n",
    "for i in np.unique(hc_labels):  # Loop for inner summation\n",
    "    X_k = X[hc_labels == i]     # Data points belonging to cluster k\n",
    "    inner_sum = np.sum(np.square(X_k - X_k.mean(axis=0)), axis=0)\n",
    "    ssw_iter.append(inner_sum)  \n",
    "ssw = np.sum(ssw_iter, axis=0)  # Outer summation\n",
    "\n",
    "# Computing SSB\n",
    "ssb_iter = []\n",
    "for i in np.unique(hc_labels):  # Loop for summation\n",
    "    X_k = X[hc_labels == i]     # Data points belonging to cluster k\n",
    "    ssb_iter.append(X_k.shape[0] * np.square(X_k.mean(axis=0) - X.mean(axis=0)))\n",
    "\n",
    "ssb = np.sum(ssb_iter, axis=0)\n",
    "\n",
    "# Verifying the formula\n",
    "np.round(sst) == np.round((ssw + ssb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the `SS_` values using Pandas / NumPy:\n",
    "\n",
    "**Now do it using pandas**\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "$x_i$ : `data`\n",
    "\n",
    "$\\overline{x}$ : `data.mean()`\n",
    "\n",
    "$\\overline{x}_k$ : `data.loc[data['labels']==k].mean()`\n",
    "\n",
    "$n_k$ : `data.loc[data['labels']==k].shape[0]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSt\n",
    "\n",
    "x_i = df_concat[metric_features]\n",
    "x_mean = df_concat[metric_features].mean()\n",
    "\n",
    "sst_sum = np.sum(np.sum(np.square(x_i - x_mean), axis=0), axis=0)\n",
    "sst_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSb\n",
    "\n",
    "ssb_sum = 0\n",
    "x_mean = df_concat[metric_features].mean()\n",
    "\n",
    "for i in df_concat['labels'].unique():\n",
    "    df_k = df_concat.loc[df_concat['labels']==i, metric_features]\n",
    "    n_k = df_k.shape[0]\n",
    "    x_k = df_k.mean()\n",
    "    ssb_sum += (n_k * np.square((x_k - x_mean)))\n",
    "\n",
    "ssb_sum = ssb_sum.sum()\n",
    "ssb_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSw\n",
    "\n",
    "ssw_sum = 0\n",
    "\n",
    "for i in df_concat['labels'].unique():\n",
    "    df_k = df_concat.loc[df_concat['labels']==i, metric_features]\n",
    "    x_k = df_k.mean()\n",
    "    ssw_sum += (np.sum(np.square(df_k - x_k ), axis=0))\n",
    "\n",
    "\n",
    "ssw_sum = np.sum(ssw_sum, axis=0)\n",
    "\n",
    "ssw_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: \n",
    "\n",
    "sst_sum == ssw_sum + ssb_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's wrap them into functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap them into functions\n",
    "\n",
    "def get_ss(df, feats):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squares (SS) for the given DataFrame.\n",
    "\n",
    "    The sum of squares is computed as the sum of the variances of each column\n",
    "    multiplied by the number of non-NA/null observations minus one.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame for which the sum of squares is to be calculated.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    df_ = df[feats]\n",
    "    ss = np.sum(df_.var() * (df_.count() - 1))\n",
    "    \n",
    "    return ss \n",
    "\n",
    "\n",
    "def get_ssb(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the between-group sum of squares (SSB) for the given DataFrame.\n",
    "    The between-group sum of squares is computed as the sum of the squared differences\n",
    "    between the mean of each group and the overall mean, weighted by the number of observations\n",
    "    in each group.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column in the DataFrame that contains the group labels.\n",
    "    \n",
    "    Returns\n",
    "    float: The between-group sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    ssb_i = 0\n",
    "    for i in np.unique(df[label_col]):\n",
    "        df_ = df.loc[:, feats]\n",
    "        X_ = df_.values\n",
    "        X_k = df_.loc[df[label_col] == i].values\n",
    "        \n",
    "        ssb_i += (X_k.shape[0] * (np.square(X_k.mean(axis=0) - X_.mean(axis=0))) )\n",
    "\n",
    "    ssb = np.sum(ssb_i)\n",
    "    \n",
    "\n",
    "    return ssb\n",
    "\n",
    "\n",
    "def get_ssw(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squared within-cluster distances (SSW) for a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column containing cluster labels.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squared within-cluster distances (SSW).\n",
    "    \"\"\"\n",
    "    feats_label = feats+[label_col]\n",
    "\n",
    "    df_k = df[feats_label].groupby(by=label_col).apply(lambda col: get_ss(col, feats), \n",
    "                                                       include_groups=False)\n",
    "\n",
    "    return df_k.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap them into functions\n",
    "\n",
    "def get_ss(df, feats):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squares (SS) for the given DataFrame.\n",
    "\n",
    "    The sum of squares is computed as the sum of the variances of each column\n",
    "    multiplied by the number of non-NA/null observations minus one.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame for which the sum of squares is to be calculated.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    df_ = df[feats]\n",
    "    ss = np.sum(df_.var() * (df_.count() - 1))\n",
    "    \n",
    "    return ss \n",
    "\n",
    "\n",
    "def get_ssb(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the between-group sum of squares (SSB) for the given DataFrame.\n",
    "    The between-group sum of squares is computed as the sum of the squared differences\n",
    "    between the mean of each group and the overall mean, weighted by the number of observations\n",
    "    in each group.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column in the DataFrame that contains the group labels.\n",
    "    \n",
    "    Returns\n",
    "    float: The between-group sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    ssb_i = 0\n",
    "    for i in np.unique(df[label_col]):\n",
    "        df_ = df.loc[:, feats]\n",
    "        X_ = df_.values\n",
    "        X_k = df_.loc[df[label_col] == i].values\n",
    "        \n",
    "        ssb_i += (X_k.shape[0] * (np.square(X_k.mean(axis=0) - X_.mean(axis=0))) )\n",
    "\n",
    "    ssb = np.sum(ssb_i)\n",
    "    \n",
    "\n",
    "    return ssb\n",
    "\n",
    "\n",
    "def get_ssw(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squared within-cluster distances (SSW) for a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column containing cluster labels.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squared within-cluster distances (SSW).\n",
    "    \"\"\"\n",
    "    feats_label = feats+[label_col]\n",
    "\n",
    "    df_k = df[feats_label].groupby(by=label_col).apply(lambda col: get_ss(col, feats), \n",
    "                                                       include_groups=False)\n",
    "\n",
    "    return df_k.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sst_ = get_ss(df_concat, metric_features)\n",
    "df_ssb_ = get_ssb(df_concat, metric_features, 'labels')\n",
    "df_ssw_ = get_ssw(df_concat, metric_features, 'labels')\n",
    "\n",
    "print(\"SSb:  \", df_ssb_)\n",
    "print(\"SSw:  \", df_ssw_)\n",
    "print(\"SSt:  \", df_sst_)\n",
    "print(\"SSt == SSb+SSw ? \", (df_sst_ == df_ssb_ + df_ssw_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$\n",
    "\n",
    "The $R^2$ is a measure of the homogeneity of a cluster solution. \n",
    "\n",
    "It is based on: \n",
    "\n",
    "\n",
    "$$\n",
    "SS_t = SS_w + SS_b\n",
    "\\\\\n",
    "$$\n",
    "<br>\n",
    "\n",
    "$$\n",
    "R^2 = \\cfrac{SS_b}{SS_t}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figures/clustering/ss_k1_kn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figures/clustering/r2_k1_kn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_rsq(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared value for a given DataFrame and features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column containing the labels or cluster assignments.\n",
    "\n",
    "    Returns:\n",
    "    float: The R-squared value, representing the proportion of variance explained by the clustering.\n",
    "    \"\"\"\n",
    "\n",
    "    df_sst_ = get_ss(df, feats)                 # get total sum of squares\n",
    "    df_ssw_ = get_ssw(df, feats, label_col)     # get ss within\n",
    "    df_ssb_ = df_sst_ - df_ssw_                 # get ss between\n",
    "\n",
    "    # r2 = ssb/sst \n",
    "    return (df_ssb_/df_sst_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the best linkage method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_hc(df, link_method, max_nclus, min_nclus=1, dist=\"euclidean\"):\n",
    "    \"\"\"This function computes the R2 for a set of cluster solutions given by the application of a hierarchical method.\n",
    "    The R2 is a measure of the homogenity of a cluster solution. It is based on SSt = SSw + SSb and R2 = SSb/SSt. \n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to apply clustering\n",
    "    link_method (str): either \"ward\", \"complete\", \"average\", \"single\"\n",
    "    max_nclus (int): maximum number of clusters to compare the methods\n",
    "    min_nclus (int): minimum number of clusters to compare the methods. Defaults to 1.\n",
    "    dist (str): distance to use to compute the clustering solution. Must be a valid distance. Defaults to \"euclidean\".\n",
    "    \n",
    "    Returns:\n",
    "    ndarray: R2 values for the range of cluster solutions\n",
    "    \"\"\"\n",
    "    \n",
    "    r2 = []  # where we will store the R2 metrics for each cluster solution\n",
    "    feats = df.columns.tolist()\n",
    "    \n",
    "    for i in range(min_nclus, max_nclus+1):  # iterate over desired ncluster range\n",
    "        cluster = AgglomerativeClustering(n_clusters=i, metric=dist, linkage=link_method)\n",
    "        \n",
    "        #get cluster labels\n",
    "        hclabels = cluster.fit_predict(df) \n",
    "        \n",
    "        # concat df with labels\n",
    "        df_concat = pd.concat([df, pd.Series(hclabels, name='labels', index=df.index)], axis=1)  \n",
    "        \n",
    "        \n",
    "        # append the R2 of the given cluster solution\n",
    "        r2.append(get_rsq(df_concat, feats, 'labels'))\n",
    "        \n",
    "    return np.array(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# THIS TAKES A FEW MINUTES TO RUN!!\n",
    "##########################################\n",
    "\n",
    "hc_methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "max_nclus = 10\n",
    "\n",
    "r2_hc = np.vstack([ get_r2_hc(df[metric_features], \n",
    "                              link, \n",
    "                              max_nclus=max_nclus, \n",
    "                              min_nclus=1, \n",
    "                              dist=\"euclidean\") \n",
    "                              for link in hc_methods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_hc_methods = pd.DataFrame(r2_hc.T, index=range(1, max_nclus + 1), columns=hc_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set()\n",
    "\n",
    "# Plot data\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "sns.lineplot(data=r2_hc_methods, linewidth=2.5, markers=[\"o\"]*4)\n",
    "\n",
    "# Finalize the plot\n",
    "plt.legend(title=\"HC methods\", title_fontsize=11)\n",
    "plt.xticks(range(1, max_nclus + 1))\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "\n",
    "fig.suptitle(\"$R^2$ plot for various hierarchical methods\", fontsize=21)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the number of clusters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is the **first big jump** on the Dendrogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 and n_clusters=None ensures we compute the full tree\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "\n",
    "\n",
    "hclust = AgglomerativeClustering(linkage=linkage, metric=distance, distance_threshold=0, n_clusters=None)\n",
    "hclust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 100\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering Dendrogram: {linkage.title()} Linkage', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'{distance.title()} Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Visualize the Dendrogram with y_threshold = 75\n",
    "##########################################\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 75\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering Dendrogram: {linkage.title()} Linkage', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'{distance.title()} Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cluster solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage = 'ward'\n",
    "distance = 'euclidean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 cluster solution\n",
    "n_clusters = 4\n",
    "\n",
    "hc4_clust = AgglomerativeClustering(linkage=linkage, metric=distance, n_clusters=n_clusters)\n",
    "hc4_labels = hc4_clust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the 4 clusters\n",
    "df_concat = pd.concat([df[metric_features], \n",
    "                       pd.Series(hc4_labels, \n",
    "                                 name='labels', \n",
    "                                 index=df.index)], \n",
    "                    axis=1)\n",
    "\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 cluster solution\n",
    "n_clusters=5\n",
    "\n",
    "hc5_clust = AgglomerativeClustering(linkage=linkage, metric=distance, n_clusters=n_clusters)\n",
    "hc5_labels = hc5_clust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the 5 clusters\n",
    "df_concat = pd.concat([df[metric_features], \n",
    "                       pd.Series(hc5_labels, \n",
    "                                 name='labels', \n",
    "                                 index=df.index)], \n",
    "                    axis=1)\n",
    "\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See crosstab of 4 vs 5\n",
    "## What does this mean?\n",
    "\n",
    "pd.crosstab(\n",
    "    pd.Series(hc5_labels, name='hc5_labels', index=df.index),\n",
    "    pd.Series(hc4_labels, name='hc4_labels', index=df.index),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Hierarchical clustering solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cluster solution\n",
    "linkage = \"ward\"\n",
    "distance = \"euclidean\"\n",
    "n_clusters = 4\n",
    "\n",
    "hclust = AgglomerativeClustering(linkage=linkage, metric=distance, n_clusters=n_clusters)\n",
    "\n",
    "hc_labels = hclust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the final clusters\n",
    "\n",
    "df_concat = pd.concat([\n",
    "    df[metric_features], \n",
    "    pd.Series(hc_labels, name='labels', index=df.index)\n",
    "    ], \n",
    "    axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Visualize the cluster means as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "hc_profile = df_concat.groupby('labels').mean().T\n",
    "\n",
    "sns.heatmap(hc_profile,\n",
    "            center=0, annot=True, cmap=\"BrBG\", fmt=\".2f\",\n",
    "            ax=ax \n",
    "            )\n",
    "\n",
    "ax.set_xlabel(\"Cluster Labels\")\n",
    "ax.set_title(\"Cluster Profiling:\\nHierarchical Clustering with 4 Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Compare the cluster means to the population means. \n",
    "\n",
    "Visualize the cluster means with the population means in the same heatmap.\n",
    "\n",
    "Explain these values for the population means.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_profile = df_concat.groupby('labels').mean().T\n",
    "\n",
    "df_means = df_concat[metric_features].mean()\n",
    "df_means.name=\"Data\"\n",
    "\n",
    "\n",
    "pd.concat([hc_profile, \n",
    "           df_means], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "hc_profile = df_concat.groupby('labels').mean().T\n",
    "\n",
    "df_means = df_concat[metric_features].mean()\n",
    "df_means.name=\"Data\"\n",
    "\n",
    "\n",
    "hc_profile = pd.concat([hc_profile, \n",
    "                        df_means], axis=1\n",
    "                        )\n",
    "\n",
    "sns.heatmap(hc_profile,\n",
    "            center=0, annot=True, cmap=\"BrBG\", fmt=\".2f\",\n",
    "            ax=ax \n",
    "            )\n",
    "\n",
    "ax.set_xlabel(\"Cluster Labels\")\n",
    "ax.set_title(\"Cluster Profiling:\\nHierarchical Clustering with 4 Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Visualize the cluster means with the cluster sizes\n",
    "\n",
    "Create a subplot with 1 row and 2 columns.\n",
    "\n",
    "In the left subplot, visualize the heatmap of cluster means, and in the right subplot visualize the cluster sizes.\n",
    "\n",
    "*Hint:* Use `sns.countplot()`.\n",
    "\n",
    "https://seaborn.pydata.org/generated/seaborn.countplot.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = None \n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5), width_ratios=[.6,.4], tight_layout=True)\n",
    "\n",
    "hc_profile = df_concat.groupby('labels').mean().T\n",
    "\n",
    "sns.heatmap(hc_profile,\n",
    "            center=0, annot=True, cmap=\"BrBG\", fmt=\".2f\",\n",
    "            ax=axes[0]\n",
    "            )\n",
    "axes[0].set_xlabel(\"Cluster Labels\")\n",
    "axes[0].set_title(\"Heatmap of Cluster Means\")\n",
    "\n",
    "\n",
    "sns.countplot(df_concat, x='labels', ax=axes[1])\n",
    "axes[1].set_title(\"Cluster Sizes\")\n",
    "axes[1].set_xlabel(\"Cluster Labels\")\n",
    "\n",
    "fig.suptitle(\"Cluster Profiling:\\nHierarchical Clustering with 4 Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "The figure below shows the dendrogram produced by using the single linkage method to cluster the datapoints given. Identify the letter in the dendrogram corresponding to each point in the scatter plot:\n",
    "\n",
    "*Example:* Point 10 = Letter L\n",
    "\n",
    "![HC Exercise](./../figures/clustering/hc_exer_01.png)\n",
    "\n",
    "Answer:\n",
    "| Dendrogram | Data | \n",
    "|---|---| \n",
    "| A | 0 | \n",
    "| B | 6 | \n",
    "| C | 1 | \n",
    "| D | 7 | \n",
    "| E | 2 | \n",
    "| F | 5 | \n",
    "| G | 3 | \n",
    "| H | 4 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More references on cluster evaluation:\n",
    "\n",
    "\n",
    "Halkidi, M., Batistakis, Y., & Vazirgiannis, M. (2002). Clustering validity checking methods. *ACM SIGMOD Record*, 31(3), 19–27. https://doi.org/10.1145/601858.601862\n",
    "\n",
    "Liu, Y., Li, Z., Xiong, H., Gao, X. & Wu, J. (2010), Understanding of Internal Clustering Validation Measures, *2010 IEEE International Conference on Data Mining*, Sydney, NSW, Australia, 2010, pp. 911-916, https://10.1109/ICDM.2010.35. \n",
    "\n",
    "\n",
    "Pandove, D., Goel, S., & Rani, R. (2018). Systematic Review of Clustering High-Dimensional and Large Datasets. *ACM Transactions on Knowledge Discovery From Data*, 12(2), 1–68. https://doi.org/10.1145/3132088\n",
    "\n",
    "\n",
    "Todeschini, R., Ballabio, D., Termopoli, V., & Consonni, V. (2024). Extended multivariate comparison of 68 cluster validity indices. A review. *Chemometrics and Intelligent Laboratory Systems*, 251, 105117. https://doi.org/10.1016/j.chemolab.2024.105117\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Session: K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

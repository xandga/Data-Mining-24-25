{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember: library imports are ALWAYS at the top of the script, no exceptions!\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "\n",
    "from itertools import product\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "# for better resolution plots\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n",
    "\n",
    "# Setting seaborn style\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context\n",
    "The data we will be using through the pratical classes comes from a small relational database whose schema can be seen below:\n",
    "![alt text](../figures/schema.png \"Relation database schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to database\n",
    "my_path = os.path.join(\"..\", \"data\", \"datamining.db\")\n",
    "\n",
    "# connect to the database\n",
    "conn = sqlite3.connect(my_path)\n",
    "\n",
    "# the query\n",
    "query = \"\"\"\n",
    "select\n",
    "    age, \n",
    "    income, \n",
    "    frq, \n",
    "    rcn, \n",
    "    mnt, \n",
    "    clothes, \n",
    "    kitchen, \n",
    "    small_appliances, \n",
    "    toys, \n",
    "    house_keeping,\n",
    "    dependents, \n",
    "    per_net_purchase,\n",
    "    g.gender, \n",
    "    e.education, \n",
    "    m.status, \n",
    "    r.description\n",
    "from customers as c\n",
    "    join genders as g on g.id = c.gender_id\n",
    "    join education_levels as e on e.id = c.education_id\n",
    "    join marital_status as m on m.id = c.marital_status_id\n",
    "    join recommendations as r on r.id = c.recommendation_id\n",
    "order by c.id;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of your original dataset\n",
    "\n",
    "why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata\n",
    "- *id* - The unique identifier of the customer\n",
    "- *age* - The year of birht of the customer\n",
    "- *income* - The income of the customer\n",
    "- *frq* - Frequency: number of purchases made by the customer\n",
    "- *rcn* - Recency: number of days since last customer purchase\n",
    "- *mnt* - Monetary: amount of € spent by the customer in purchases\n",
    "- *clothes* - Number of clothes items purchased by the customer\n",
    "- *kitchen* - Number of kitchen items purchased by the customer\n",
    "- *small_appliances* - Number of small_appliances items purchased by the customer\n",
    "- *toys* - Number of toys items purchased by the customer\n",
    "- *house_keeping* - Number of house_keeping items purchased by the customer\n",
    "- *dependents* - Binary. Whether or not the customer has dependents\n",
    "- *per_net_purchase* - Percentage of purchases made online\n",
    "- *education* - Education level of the customer\n",
    "- *status* - Marital status of the customer\n",
    "- *gender* - Gender of the customer\n",
    "- *description* - Last customer's recommendation description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems:\n",
    "- Duplicates?\n",
    "- Data types?\n",
    "- Missing values?\n",
    "- Strange values?\n",
    "- Descriptive statistics?\n",
    "\n",
    "### Take a closer look and point out possible problems:\n",
    "\n",
    "(hint: a missing values in pandas is represented with a NaN value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicating modifications from previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"\" by nans\n",
    "df.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# count of missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix wrong dtypes\n",
    "df.dependents = df.dependents.astype(\"boolean\")  # converting to \"boolean\" over \"bool\" allows preservation of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset data types again\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check descriptive statistics again\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define metric and non-metric features. Why?\n",
    "non_metric_features = [\"education\", \"status\", \"gender\", \"dependents\", \"description\"]\n",
    "metric_features = df.columns.drop(non_metric_features).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values\n",
    "\n",
    "https://statisticsbyjim.com/basics/missing-data/\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "Read: Section 2.4.2 \n",
    "\n",
    "Han, J., Pei, J., & Tong, H. (2022). *Data mining: Concepts and Techniques* (4th ed.). Morgan Kaufmann.\n",
    "\n",
    "<br>\n",
    "\n",
    "Read: Chapter 4\n",
    "\n",
    "García, S., Luengo, J., & Herrera, F. (2014). *Data preprocessing in data mining*. Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we fill missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of central tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy to apply central tendency measures imputation\n",
    "df_central = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of missing values\n",
    "df_central.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df_central[metric_features].mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = df_central[metric_features].median()\n",
    "medians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean or Median?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVZ3NMFVasiZ"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "## You don't need to understand this code\n",
    "#####################\n",
    "\n",
    "\n",
    "skew_demo = np.round(skewnorm.rvs(180, size=1500, random_state=68410237)*100, 0).astype(int)\n",
    "\n",
    "skew_md = np.median(skew_demo)\n",
    "skew_mn = np.mean(skew_demo)\n",
    "\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "sns.histplot(skew_demo, bins=20,\n",
    "             ax=ax, color='tab:orange')\n",
    "\n",
    "ax.axvline(skew_md, color='black', linewidth=4,\n",
    "           label='Median: {}'.format(int(skew_md)))\n",
    "ax.axvline(skew_mn, color='blue', linestyle='dashed', linewidth=4,\n",
    "           label='Mean: {}'.format(int(skew_mn)))\n",
    "\n",
    "ax.legend(handlelength=5)\n",
    "\n",
    "ax.set_title(\"Histogram\")\n",
    "plt.show()\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about non-numeric features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each step of these commands\n",
    "modes = df_central[non_metric_features].mode().loc[0]\n",
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs on df_central\n",
    "# CODE HERE\n",
    "\n",
    "df_central[metric_features] = df_central[metric_features].fillna(medians)\n",
    "df_central[non_metric_features] = df_central[non_metric_features].fillna(modes)\n",
    "\n",
    "df_central.isna().sum()  # checking how many NaNs we still have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new df copy to explore neighbordhood imputation\n",
    "df_neighbors = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing rows with NaNs\n",
    "nans_index = df_neighbors.isna().any(axis=1)\n",
    "df_neighbors[nans_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNImputer - only works for numerical variables. Fill NaNs on df_neighbors\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "df_neighbors[metric_features] = imputer.fit_transform(df_neighbors[metric_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See rows with NaNs imputed\n",
    "# CODE HERE\n",
    "df_neighbors[nans_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's keep the central imputation\n",
    "df = df_central.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An overview of our previous data exploration\n",
    "\n",
    "You can also explore this dataset using the exported `pandas-profiling` report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](https://raw.githubusercontent.com/fpontejos/Data-Mining-24-25/refs/heads/main/figures/eda/categorical_variables_counts.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/fpontejos/Data-Mining-24-25/refs/heads/main/figures/eda/numeric_variables_histograms.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/fpontejos/Data-Mining-24-25/refs/heads/main/figures/eda/numeric_variables_boxplots.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/fpontejos/Data-Mining-24-25/refs/heads/main/figures/eda/pairwise_numeric_scatterplots.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/fpontejos/Data-Mining-24-25/refs/heads/main/figures/eda/correlation_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "\n",
    "Why do we need to remove outliers? Which methods can we use?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal using only the IQR method\n",
    "\n",
    "Why should you use/not use this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTg3mjGfa487"
   },
   "source": [
    "Inter-Quartile Range formula:\n",
    "\n",
    "<br>\n",
    "\n",
    "$IQR = Q_3 – Q_1$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**Outliers** are the observations that fall\n",
    "- below $Q_1 − 1.5 \\times IQR$\n",
    "\n",
    "or\n",
    "\n",
    "- above $Q_3 + 1.5 \\times IQR$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "https://online.stat.psu.edu/stat200/lesson/3/3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the IQR\n",
    "# HINT: Use .quantile()\n",
    "\n",
    "# Calculate Q1, Q3, IQR\n",
    "\n",
    "q1 = df[metric_features].quantile(.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1, Q3, IQR\n",
    "\n",
    "q3 = df[metric_features].quantile(.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1, Q3, IQR\n",
    "\n",
    "iqr = (q3 - q1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute upper and lower limit \n",
    "\n",
    "# lower_limit = Q1 -1.5*IQR\n",
    "lower_lim = q1 - (1.5 * iqr)\n",
    "\n",
    "# upper_limit = Q3 + 1.5*IQR\n",
    "upper_lim = q3 + (1.5 * iqr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_iqr = []\n",
    "for metric in metric_features:\n",
    "    llim = lower_lim[metric]\n",
    "    ulim = upper_lim[metric]\n",
    "    filters_iqr.append(df[metric].between(llim, ulim, inclusive='neither'))\n",
    "\n",
    "filters_iqr_all = pd.Series(np.all(filters_iqr, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters_iqr_all.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iqr = df[filters_iqr_all]\n",
    "print('Percentage of data kept after removing outliers:', 100*(np.round(df_iqr.shape[0] / df_original.shape[0], 4)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3foxpUT9bvYv"
   },
   "source": [
    "What do you think of this number?\n",
    "\n",
    "In general we want to keep as much of our data as possible.\n",
    "\n",
    "As a rule of thumb, try not to remove more than 5% of your rows. \n",
    "\n",
    "***This is only a rule of thumb!*** \n",
    "\n",
    "In some cases it really is necessary to remove more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual outlier removal\n",
    "\n",
    "Now let's try \"manually\" filtering the dataset's outliers\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Let's look at the boxplots again:\n",
    "\n",
    "![](https://raw.githubusercontent.com/fpontejos/Data-Mining-24-25/refs/heads/main/figures/eda/numeric_variables_boxplots.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may vary from session to session, and is prone to varying interpretations.\n",
    "# A simple example is provided below:\n",
    "\n",
    "# ( df['feature1']<= limit1 ) & ( df['feature2']<= limit2 ) ...\n",
    "\n",
    "\n",
    "filters_man = (\n",
    "    (df['house_keeping']<=50)\n",
    "    &\n",
    "    (df['kitchen']<=40)\n",
    "    &\n",
    "    (df['toys']<=35)\n",
    "    &\n",
    "    (df['education']!='OldSchool')\n",
    ")\n",
    "\n",
    "df_man = df[filters_man]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of data kept after removing outliers:', 100*np.round(df_man.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining different outlier methods\n",
    "\n",
    "More robust/ consistent outlier detection method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What is this doing?\n",
    "df_out = df[(filters_iqr_all | filters_man)] \n",
    "\n",
    "\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_out.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the manual filtering version\n",
    "\n",
    "df = df_man.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier threshold value\n",
    "\n",
    "You may change the values of observations for a given variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember the 'rcn' variable had an odd behavior? \n",
    "# This is the time to fix that\n",
    "\n",
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df['rcn']>100).value_counts())\n",
    "\n",
    "rcn_t = df['rcn'].copy()\n",
    "rcn_t.loc[rcn_t>100] = 100\n",
    "\n",
    "df['rcn'] = rcn_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Visualize your boxplots and histograms before and after outlier removal.\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/fpontejos/Data-Mining-24-25/refs/heads/main/figures/eda/boxplots_before_after_outliers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "A reminder of our metadata:\n",
    "- *id* - The unique identifier of the customer\n",
    "- *age* - The year of birht of the customer\n",
    "- *income* - The income of the customer\n",
    "- *frq* - Frequency: number of purchases made by the customer\n",
    "- *rcn* - Recency: number of days since last customer purchase\n",
    "- *mnt* - Monetary: amount of € spent by the customer in purchases\n",
    "- *clothes* - Number of clothes items purchased by the customer\n",
    "- *kitchen* - Number of kitchen items purchased by the customer\n",
    "- *small_appliances* - Number of small_appliances items purchased by the customer\n",
    "- *toys* - Number of toys items purchased by the customer\n",
    "- *house_keeping* - Number of house_keeping items purchased by the customer\n",
    "- *dependents* - Binary. Whether or not the customer has dependents\n",
    "- *per_net_purchase* - Percentage of purchases made online\n",
    "- *education* - Education level of the customer\n",
    "- *status* - Marital status of the customer\n",
    "- *gender* - Gender of the customer\n",
    "- *description* - Last customer's recommendation description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to pull as many features as you can; You can always drop them later on\n",
    "# You can start by converting the 'age' variable to the actual age\n",
    "# You can also store the birth year in another feature\n",
    "\n",
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['birth_year'] = df['age']\n",
    "df['age'] = 2024 - df['birth_year']\n",
    "new_features.append('birth_year')\n",
    "\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spent_online'] = df['per_net_purchase']*df['mnt']/100\n",
    "\n",
    "new_features.append('spent_online')\n",
    "\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable selection: Redundancy VS Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundancy: Handling highly correlated variables\n",
    "\n",
    "We already saw our original correlation matrix:\n",
    "\n",
    "![](https://raw.githubusercontent.com/fpontejos/Data-Mining-24-25/refs/heads/main/figures/eda/correlation_matrix.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables according to their correlations\n",
    "# Updating metric_features\n",
    "\n",
    "## RUN THIS CELL ONLY ONCE!!!!\n",
    "\n",
    "# CODE HERE\n",
    "metric_features.append(\"spent_online\")\n",
    "metric_features.remove(\"mnt\")\n",
    "metric_features.remove(\"age\")\n",
    "\n",
    "metric_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also drop the features from the df\n",
    "# But sometimes they may be useful for cluster profiling later\n",
    "\n",
    "# df.drop(..., inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevancy\n",
    "Selecting variables based on the relevancy of each one to the task. \n",
    "\n",
    "Example: \n",
    "- remove uncorrelated variables with the target,\n",
    "- stepwise regression,\n",
    "- use variables for product clustering,\n",
    "- use variables for socio-demographic clustering,\n",
    "- ...\n",
    "\n",
    "Variables that aren't correlated with any other variable are often also not relevant. In this case we will not focus on this a lot since we don't have a defined task yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo data exploration\n",
    "\n",
    "Check if the data looks the way you expect it to. \n",
    "\n",
    "- Have you missed some outliers? \n",
    "- Are there still missing values?\n",
    "- Is the data normalized?\n",
    "\n",
    "This is an iterative process. It is likely you will change your preprocessing steps frequently throughout your group work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember our original data\n",
    "\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljRWLlWXOUOg"
   },
   "source": [
    "### MinMax Scaling\n",
    "\n",
    "Transforms values to be between [0,1]\n",
    "\n",
    "$\n",
    "x' = \\frac{x - min(X)}{max(X) - min(X)}\n",
    "$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "where:\n",
    "\n",
    "$x$ is an original value\n",
    "\n",
    "$x'$ is the normalized value\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGv_DMYuM57c"
   },
   "outputs": [],
   "source": [
    "## Let's look at an example:\n",
    "\n",
    "demo_ages = pd.DataFrame({\"Age\": [14, 24, 22, 18, 18, 26, 11, 23, 13, 12]})\n",
    "demo_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqaegyH4OH6U"
   },
   "outputs": [],
   "source": [
    "\n",
    "# How to do this with Python/Pandas?\n",
    "#  ( demo_ages - min(demo_ages) ) \n",
    "# --------------------------------- \n",
    "#  max(demo_ages) - min(demo_ages)\n",
    "\n",
    "demo_ages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_age = demo_ages[\"Age\"] # finish this line\n",
    "max_age = demo_ages[\"Age\"] # finish this line\n",
    "\n",
    "mm_age = None # finish this line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jfF8iYRPjb5"
   },
   "source": [
    "Remember the formula:\n",
    "\n",
    "$\n",
    "x' = \\frac{x - min(X)}{max(X) - min(X)}\n",
    "$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "where:\n",
    "\n",
    "$x$ is an original value\n",
    "\n",
    "$x'$ is the normalized value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7HQkHP-PtjW"
   },
   "source": [
    "(This is not showing all rows)\n",
    "\n",
    "\n",
    "Age (Orig) | Formula        | Result\n",
    "-----------|----------------|-------\n",
    "14         | (14 - 11) / 15 | 0.2\n",
    "24         | (24 - 11) / 15 | 0.86666667\n",
    "26         | (26 - 11) / 15 | 1\n",
    "11         | (11 - 11) / 15 | 0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZaCXNcRQdUI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qpW1aEnQ2BU"
   },
   "outputs": [],
   "source": [
    "# sklearn makes it even easier:\n",
    "\n",
    "demo_ages[\"Age_minmax\"] = MinMaxScaler()# finish this line\n",
    "\n",
    "demo_ages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now do it for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MinMaxScaler to scale the data\n",
    "mm_scaler = MinMaxScaler()\n",
    "mm_scaled_feat = None # finish this line\n",
    "mm_scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the fit method is doing (notice the trailing underscore):\n",
    "print(\"Parameters fitted:\\n\")\n",
    "pd.DataFrame(\n",
    "    [mm_scaler.data_min_, mm_scaler.data_max_], \n",
    "    columns=metric_features, \n",
    "    index=['min','max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace original metric_features values with mm_scaled_feat values\n",
    "df_minmax[metric_features] = mm_scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking max and min of minmaxed variables\n",
    "df_minmax[metric_features].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax[metric_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[metric_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46jI-WiEOW1s"
   },
   "source": [
    "### Standard Scaling\n",
    "\n",
    "AKA Z-Score Scaling\n",
    "\n",
    "Standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "The standard score of a sample x is calculated as:\n",
    "\n",
    "$z = (x - u) / s$\n",
    "\n",
    "where:\n",
    "\n",
    "$u$ is the mean of the training samples,\n",
    "\n",
    "$s$ is the standard deviation of the training samples\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weILbQL5SGIF"
   },
   "outputs": [],
   "source": [
    "## Let's see the age demo again\n",
    "\n",
    "demo_ages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HN5l8dOSJXd"
   },
   "source": [
    "$z = (x - u) / s$\n",
    "\n",
    "where:\n",
    "\n",
    "$u$ is the mean of the training samples,\n",
    "\n",
    "$s$ is the standard deviation of the training samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## u = mean of age\n",
    "## s = std of age\n",
    "\n",
    "# How to do this in Python/Pandas?\n",
    "# ( demo_ages - mean(demo_ages) )\n",
    "# -------------------------------\n",
    "#        std(demo_ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr7B4yY8STxp"
   },
   "outputs": [],
   "source": [
    "## u = mean of age\n",
    "## s = std of age\n",
    "\n",
    "mean_age = demo_ages[\"Age\"].mean()\n",
    "std_age  = demo_ages[\"Age\"].std(ddof = 0) # to match sklearn implementation of std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(demo_ages[\"Age\"] - mean_age) / std_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5kPgsg_Sp5L"
   },
   "outputs": [],
   "source": [
    "# Or use sklearn:\n",
    "\n",
    "demo_ages[\"Age_standard\"] = StandardScaler().fit_transform(demo_ages[[\"Age\"]])\n",
    "\n",
    "demo_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vjktwnqTrp3"
   },
   "outputs": [],
   "source": [
    "demo_ages.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now do it for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_scaler = None # finish this line\n",
    "ss_scaled_feat = None # finish this line\n",
    "ss_scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the fit method is doing (notice the trailing underscore):\n",
    "print(\"Parameters fitted:\\n\")\n",
    "pd.DataFrame([ss_scaler.mean_, np.sqrt(ss_scaler.var_)], columns=metric_features, index=['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard[metric_features] = ss_scaled_feat\n",
    "df_standard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking mean and variance of standardized variables\n",
    "df_standard[metric_features].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: What if we had a training and test set? Should we fit a Scaler in both? What about other Sklearn objects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare MinMaxScaler vs StandardScaler vs Original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "fig, axes = plt.subplots(2,3, figsize=(8,5), tight_layout=True, sharex='col', sharey='row')\n",
    "\n",
    "bp_feat_l = 'house_keeping'\n",
    "\n",
    "sns.boxplot(df_original, x=bp_feat_l, ax=axes[0][0], width=.4)\n",
    "axes[0][0].set_title('Original')\n",
    "axes[0][0].set_ylabel(bp_feat_l)\n",
    "\n",
    "sns.boxplot(df_minmax, x=bp_feat_l, ax=axes[0][1], width=.4)\n",
    "axes[0][1].set_title('MinMaxScaler()')\n",
    "\n",
    "sns.boxplot(df_standard, x=bp_feat_l, ax=axes[0][2], width=.4)\n",
    "axes[0][2].set_title('StandardScaler()')\n",
    "\n",
    "\n",
    "\n",
    "bp_feat_r = 'income'\n",
    "\n",
    "sns.boxplot(df_original, x=bp_feat_r, ax=axes[1][0], width=.4)\n",
    "axes[1][0].set_ylabel(bp_feat_r)\n",
    "\n",
    "sns.boxplot(df_minmax, x=bp_feat_r, ax=axes[1][1], width=.4)\n",
    "\n",
    "sns.boxplot(df_standard, x=bp_feat_r, ax=axes[1][2], width=.4)\n",
    "\n",
    "\n",
    "axes[1][0].set_xlabel(None)\n",
    "axes[1][1].set_xlabel(None)\n",
    "axes[1][2].set_xlabel(None)\n",
    "\n",
    "fig.suptitle('Boxplots: \"{}\" and \"{}\"'.format(bp_feat_l, bp_feat_r))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "What about histograms?\n",
    "\n",
    "**Exercise**: Visualize histograms here, compare the shapes of the original distribution with the MinMaxScaler and StandardScaler results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create subplots with 3 rows, 1 column:\n",
    "fig, axes = plt.subplots(3,1, \n",
    "                         figsize=(5,5), tight_layout=True)\n",
    "\n",
    "# Which feature do you want to visualize?\n",
    "hp_feat = 'house_keeping'\n",
    "\n",
    "# common function args for histplot\n",
    "hp_args = dict(x=hp_feat, bins=15)\n",
    "\n",
    "\n",
    "sns.histplot(df_original, ax=axes[0], **hp_args)\n",
    "axes[0].set_title('{}: Original'.format(hp_feat))\n",
    "axes[0].set_xlabel(None)\n",
    "\n",
    "sns.histplot(df_minmax, ax=axes[1], **hp_args)\n",
    "axes[1].set_title('{}: MinMaxScaler()'.format(hp_feat))\n",
    "axes[1].set_xlabel(None)\n",
    "\n",
    "sns.histplot(df_standard, ax=axes[2], **hp_args)\n",
    "axes[2].set_title('{}: StandardScaler()'.format(hp_feat))\n",
    "axes[2].set_xlabel(None)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_standard.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohc = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's remove status=Whatever\n",
    "df_ohc.loc[df_ohc['status'] == 'Whatever', 'status'] = df['status'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical features. Get feature names and create a DataFrame \n",
    "# with the one-hot encoded categorical features (pass feature names)\n",
    "ohc = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
    "ohc_feat = ohc.fit_transform(df_ohc[non_metric_features])\n",
    "ohc_feat_names = ohc.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names and create a DataFrame \n",
    "# with the one-hot encoded categorical features (pass feature names)\n",
    "ohc_feat = ohc.fit_transform(df_ohc[non_metric_features])\n",
    "ohc_feat_names = ohc.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohc_df = pd.DataFrame(ohc_feat, index=df_ohc.index, columns=ohc_feat_names)  # Why the index=df_ohc.index?\n",
    "ohc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain ohc variables\n",
    "df_ohc = pd.concat([df_ohc, ohc_df], axis=1)\n",
    "df_ohc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ohc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also choose to drop the original non_metric_features from your df\n",
    "# df.drop(columns=non_metric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename OHE columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename OHE columns from \"feature_val_a\" to \"oh_feature_val_a\"\n",
    "## e.g. status_Widow to oh_status_Widow\n",
    "## We do this to be able to distinguish the OHE columns more easily later\n",
    "\n",
    "## Assemble OHE columns and their new column names\n",
    "rename_ohe_cols = {}\n",
    "\n",
    "for i in non_metric_features:\n",
    "    for j in df.columns[df.columns.str.startswith(i)].to_list() :\n",
    "        if j not in non_metric_features:\n",
    "            rename_ohe_cols[j] = 'oh_' + j\n",
    "\n",
    "df.rename(columns=rename_ohe_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo data exploration\n",
    "\n",
    "Check if the data looks the way you expect it to. \n",
    "\n",
    "- Have you missed some outliers? \n",
    "- Are there still missing values?\n",
    "- Is the data normalized?\n",
    "\n",
    "This is an iterative process. It is likely you will change your preprocessing steps frequently throughout your group work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A tool to assist you through your exploratory data analysis\n",
    "\n",
    "Optionally, you may use `pandas-profiling` in your data analysis. \n",
    "\n",
    "Remember, although this tool provides excelent insights about the data you're working with, it is not enough to perform a proper analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ProfileReport(\n",
    "#     df,\n",
    "#     title='Tugas Customer Data Preprocessed',\n",
    "#     correlations={\n",
    "#         \"pearson\": {\"calculate\": True},\n",
    "#         \"spearman\": {\"calculate\": False},\n",
    "#         \"kendall\": {\"calculate\": False},\n",
    "#         \"phi_k\": {\"calculate\": False},\n",
    "#         \"cramers\": {\"calculate\": False},\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

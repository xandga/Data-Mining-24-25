{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional: install `graphviz`**\n",
    "\n",
    "To produce the decision tree visualization you should install the graphviz package into your system: \n",
    "\n",
    "https://stackoverflow.com/questions/35064304/runtimeerror-make-sure-the-graphviz-executables-are-on-your-systems-path-aft\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one of these in case you have problems with graphviz\n",
    "\n",
    "# All users: try this first\n",
    "# ! conda install graphviz\n",
    "\n",
    "# If that doesn't work:\n",
    "# Ubuntu/Debian users only\n",
    "# ! sudo apt-get update && sudo apt-get install graphviz\n",
    "\n",
    "# Mac users only (assuming you have homebrew installed)\n",
    "# ! brew install graphviz\n",
    "\n",
    "# Windows users, check the stack overflow link. Sorry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you have graphviz installed\n",
    "# import graphviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join('..', 'data', 'data_preprocessed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature names into groups\n",
    "# Remember which metric_features we decided to keep?\n",
    "metric_features = ['income',\n",
    " 'frq',\n",
    " 'rcn',\n",
    " 'clothes',\n",
    " 'kitchen',\n",
    " 'small_appliances',\n",
    " 'toys',\n",
    " 'house_keeping',\n",
    " 'per_net_purchase',\n",
    " 'spent_online']\n",
    "\n",
    "non_metric_features = df.columns[df.columns.str.startswith('oh_')].tolist() # CODE HERE\n",
    "pc_features = df.columns[df.columns.str.startswith('PC')].tolist()  # CODE HERE\n",
    "\n",
    "unused_features = [i for i in df.columns if i not in (metric_features+non_metric_features+pc_features) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('metric_features:', metric_features)\n",
    "print('\\nnon_metric_features:', non_metric_features)\n",
    "print('\\nunused_features:', unused_features)\n",
    "print('\\npc_features:', pc_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we proceed\n",
    "\n",
    "- Consider applying the outlier filtering method discussed last class.\n",
    "    - We manually filtered the dataset's outliers based on a univariate analysis\n",
    "- Consider dropping/transforming the variable \"rcn\". Why?\n",
    "    - Very little correlation with any other variables\n",
    "    - Remember the Component planes: the SOM's units were indistinguishable on this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the hyperparameters found in the previous class\n",
    "dbscan = DBSCAN(eps=1.9, min_samples=20, n_jobs=4)\n",
    "dbscan_labels = dbscan.fit_predict(df[metric_features])\n",
    "Counter(dbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the newly detected outliers (they will be classified later based on the final clusters)\n",
    "df_out = df[dbscan_labels==-1].copy()\n",
    "\n",
    "# New df without outliers and 'rcn'\n",
    "df = df[dbscan_labels!=-1].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.shape, df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update features lists\n",
    "\n",
    "metric_features.remove(\"rcn\")\n",
    "unused_features.append(\"rcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features, unused_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering by Perspectives\n",
    "- Demographic/Behavioral Perspective:\n",
    "- Product Perspective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split variables into perspectives (example, requires critical thinking and domain knowledge)\n",
    "demographic_features = [\n",
    "    'income',\n",
    "    'frq',\n",
    "    'per_net_purchase',\n",
    "    'spent_online'\n",
    "]\n",
    "\n",
    "preference_features = [\n",
    "    'clothes', \n",
    "    'kitchen', \n",
    "    'small_appliances',\n",
    "    'toys', \n",
    "    'house_keeping', \n",
    "]\n",
    "\n",
    "df_dem = df[demographic_features].copy()\n",
    "df_prf = df[preference_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on K-means and Hierarchical clustering\n",
    "Based on (1) our previous tests and (2) the context of this problem, the optimal number of clusters is expected to be between 3 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the clusterers\n",
    "kmeans = KMeans(\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    metric='euclidean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from Lab 09\n",
    "\n",
    "def get_ss(df, feats):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squares (SS) for the given DataFrame.\n",
    "\n",
    "    The sum of squares is computed as the sum of the variances of each column\n",
    "    multiplied by the number of non-NA/null observations minus one.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame for which the sum of squares is to be calculated.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    df_ = df[feats]\n",
    "    ss = np.sum(df_.var() * (df_.count() - 1))\n",
    "    \n",
    "    return ss \n",
    "\n",
    "\n",
    "def get_ssb(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the between-group sum of squares (SSB) for the given DataFrame.\n",
    "    The between-group sum of squares is computed as the sum of the squared differences\n",
    "    between the mean of each group and the overall mean, weighted by the number of observations\n",
    "    in each group.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column in the DataFrame that contains the group labels.\n",
    "    \n",
    "    Returns\n",
    "    float: The between-group sum of squares of the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    ssb_i = 0\n",
    "    for i in np.unique(df[label_col]):\n",
    "        df_ = df.loc[:, feats]\n",
    "        X_ = df_.values\n",
    "        X_k = df_.loc[df[label_col] == i].values\n",
    "        \n",
    "        ssb_i += (X_k.shape[0] * (np.square(X_k.mean(axis=0) - X_.mean(axis=0))) )\n",
    "\n",
    "    ssb = np.sum(ssb_i)\n",
    "    \n",
    "\n",
    "    return ssb\n",
    "\n",
    "\n",
    "def get_ssw(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the sum of squared within-cluster distances (SSW) for a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list of str): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column containing cluster labels.\n",
    "\n",
    "    Returns:\n",
    "    float: The sum of squared within-cluster distances (SSW).\n",
    "    \"\"\"\n",
    "    feats_label = feats+[label_col]\n",
    "\n",
    "    df_k = df[feats_label].groupby(by=label_col).apply(lambda col: get_ss(col, feats), \n",
    "                                                       include_groups=False)\n",
    "\n",
    "    return df_k.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from Lab 09\n",
    " \n",
    "def get_rsq(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate the R-squared value for a given DataFrame and features.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    feats (list): A list of feature column names to be used in the calculation.\n",
    "    label_col (str): The name of the column containing the labels or cluster assignments.\n",
    "\n",
    "    Returns:\n",
    "    float: The R-squared value, representing the proportion of variance explained by the clustering.\n",
    "    \"\"\"\n",
    "\n",
    "    df_sst_ = get_ss(df, feats)                 # get total sum of squares\n",
    "    df_ssw_ = get_ssw(df, feats, label_col)     # get ss within\n",
    "    df_ssb_ = df_sst_ - df_ssw_                 # get ss between\n",
    "\n",
    "    # r2 = ssb/sst \n",
    "    return (df_ssb_/df_sst_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_scores(df, feats, clusterer, min_k=2, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        df_concat = pd.concat([df, \n",
    "                               pd.Series(labels, name='labels', index=df.index)], axis=1)  \n",
    "\n",
    "        r2_clust[n] = get_rsq(df_concat, feats, 'labels' )\n",
    "    return r2_clust\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal clusterer on demographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dem[demographic_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on demographic variables\n",
    "# This may take some time to run\n",
    "\n",
    "r2_scores = {}\n",
    "\n",
    "r2_scores['kmeans'] = get_r2_scores(df_dem, demographic_features, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores[linkage] = get_r2_scores(\n",
    "        df_dem,                 # data\n",
    "        demographic_features,   # features of perspective\n",
    "        # use HClust, changing the linkage at each iteration\n",
    "        hierarchical.set_params(linkage=linkage) \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores_df = pd.DataFrame(r2_scores)\n",
    "r2_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on demographic variables\n",
    "r2_scores_df.plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Demographic Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the process for product variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on preference variables\n",
    "r2_scores_pref = {}\n",
    "r2_scores_pref['kmeans'] = get_r2_scores(df_prf, preference_features, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores_pref[linkage] = get_r2_scores(\n",
    "        df_prf, preference_features, hierarchical.set_params(linkage=linkage)\n",
    "    )\n",
    "\n",
    "# Visualizing the R² scores for each cluster solution on product variables\n",
    "pd.DataFrame(r2_scores_pref).plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Preference Variables:\\nR2 plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the Perspectives\n",
    "- How can we merge different cluster solutions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
    "kmeans_pref = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "prod_labels = kmeans_pref.fit_predict(df_prf)\n",
    "\n",
    "kmeans_behav = KMeans(\n",
    "    n_clusters=4,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "behavior_labels = kmeans_behav.fit_predict(df_dem)\n",
    "\n",
    "df['product_labels'] = prod_labels\n",
    "df['behavior_labels'] = behavior_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count label frequencies (contigency table)\n",
    "\n",
    "pd.crosstab(df['behavior_labels'],\n",
    "            df['product_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any problems here?\n",
    "\n",
    "- too many clusters\n",
    "- clusters with few points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual merging: Merge lowest frequency clusters into closest clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centroids of clusters\n",
    "df_centroids = df.groupby(['behavior_labels', 'product_labels'])\\\n",
    "    [metric_features].mean()\n",
    "\n",
    "\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clusters with low frequency to be merged:\n",
    "# (behavior_label, product_label)\n",
    "to_merge = [(3,2), (3,0), (2,2), (0,2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the euclidean distance matrix between the centroids\n",
    "centroid_dists = euclidean = pairwise_distances(df_centroids)\n",
    "\n",
    "df_dists = pd.DataFrame(\n",
    "    centroid_dists, \n",
    "    columns=df_centroids.index, \n",
    "    index=df_centroids.index\n",
    ")\n",
    "\n",
    "df_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging each low frequency clustering (source) \n",
    "# to the closest cluster (target)\n",
    "\n",
    "source_target = {}\n",
    "\n",
    "for clus in to_merge:\n",
    "    # If cluster to merge (source) has not yet been used as target\n",
    "    if clus not in source_target.values():\n",
    "        # Add this cluster to source_target map as key\n",
    "        # Use the cluster with the smallest distance to it as value\n",
    "        source_target[clus] = df_dists.loc[clus].sort_values().index[1]\n",
    "\n",
    "source_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "# Changing the behavior_labels and product_labels based on source_target\n",
    "for source, target in source_target.items():\n",
    "    mask = (df_['behavior_labels']==source[0]) & (df_['product_labels']==source[1])\n",
    "    df_.loc[mask, 'behavior_labels'] = target[0]\n",
    "    df_.loc[mask, 'product_labels'] = target[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New contigency table\n",
    "\n",
    "pd.crosstab(df_['behavior_labels'],\n",
    "            df_['product_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original contigency table\n",
    "\n",
    "pd.crosstab(df['behavior_labels'],\n",
    "            df['product_labels'])\n",
    "\n",
    "# (3,0) was target for (3,2), so was not moved\n",
    "# (0,2) was target for (2,2), so was not moved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging using Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids of the concatenated cluster labels\n",
    "df_centroids = df.groupby(['behavior_labels', 'product_labels'])\\\n",
    "    [metric_features].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hierarchical clustering to merge the concatenated cluster centroids\n",
    "linkage = 'ward'\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage=linkage, \n",
    "    metric='euclidean', \n",
    "    distance_threshold=0, \n",
    "    n_clusters=None\n",
    ")\n",
    "\n",
    "hclust_labels = hclust.fit_predict(df_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "\n",
    "y_threshold = 2.3\n",
    "# y_threshold = 3.3\n",
    "\n",
    "dendrogram(linkage_matrix, \n",
    "           truncate_mode='level', \n",
    "           labels=df_centroids.index, p=5, \n",
    "           color_threshold=y_threshold, \n",
    "           above_threshold_color='k')\n",
    "\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    metric='euclidean', \n",
    "    n_clusters=7\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)\n",
    "df_centroids['hclust_labels'] = hclust_labels\n",
    "\n",
    "df_centroids  # centroid's cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper between concatenated clusters and hierarchical clusters\n",
    "cluster_mapper = df_centroids['hclust_labels'].to_dict()\n",
    "cluster_mapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df.copy()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "df_['merged_labels'] = df_.apply(\n",
    "    lambda row: cluster_mapper[\n",
    "        (row['behavior_labels'], row['product_labels'])\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "df_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged cluster centroids\n",
    "df_.groupby('merged_labels').mean(numeric_only=True)[metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cluster contigency table\n",
    "# Getting size of each final cluster\n",
    "df_counts = df_.groupby('merged_labels')\\\n",
    "    .size()\\\n",
    "    .to_frame()\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the product and behavior labels\n",
    "df_counts = df_counts\\\n",
    "    .rename({v:k for k, v in cluster_mapper.items()})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts['behavior_labels'] = df_counts['merged_labels'].apply(lambda x: x[0])\n",
    "df_counts['product_labels'] = df_counts['merged_labels'].apply(lambda x: x[1])\n",
    "\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts.pivot(values=0, index='behavior_labels', columns='product_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting df to have the final product, behavior and merged clusters\n",
    "df = df_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, \n",
    "                     cmap=\"tab10\",\n",
    "                     compare_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    \n",
    "    if compare_titles == None:\n",
    "        compare_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), \n",
    "                             ncols=2, \n",
    "                             figsize=figsize, \n",
    "                             constrained_layout=True,\n",
    "                             squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compare_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, \n",
    "                                            label, \n",
    "                                            color = sns.color_palette(cmap),\n",
    "                                            ax=ax[0])\n",
    "\n",
    "\n",
    "\n",
    "        sns.barplot(x=label, \n",
    "                    hue=label,\n",
    "                    y=\"counts\", \n",
    "                    data=counts, \n",
    "                    ax=ax[1], \n",
    "                    palette=sns.color_palette(cmap),\n",
    "                    legend=False\n",
    "                    )\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), \n",
    "                              rotation=40,\n",
    "                              ha='right'\n",
    "                              )\n",
    "        \n",
    "        ax[0].legend(handles, cluster_labels,\n",
    "                     loc='center left', bbox_to_anchor=(1, 0.5), title=label\n",
    "                     ) # Adaptable to number of clusters\n",
    "        \n",
    "        ax[1].set_xticks([i for i in range(len(handles))])\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profiling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profilling each cluster (product, behavior, merged)\n",
    "cluster_profiles(\n",
    "    df = df[metric_features + ['product_labels', 'behavior_labels', 'merged_labels']], \n",
    "    label_columns = ['product_labels', 'behavior_labels', 'merged_labels'], \n",
    "    figsize = (28, 13), \n",
    "    compare_titles = [\"Product clustering\", \"Behavior clustering\", \"Merged clusters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: profiling with unused / categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_metric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_educ = df[['merged_labels',\n",
    "            'oh_education_2nd Cycle',\n",
    "            'oh_education_Graduation',\n",
    "            'oh_education_Master',\n",
    "            'oh_education_PhD']].groupby(['merged_labels']).sum()\n",
    "\n",
    "df_educ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['merged_labels']].groupby(['merged_labels']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, \n",
    "                         df['merged_labels'].nunique() + 1, # Add an extra ax for population countplot\n",
    "                         figsize=(16,4),\n",
    "                         tight_layout=True,\n",
    "                        #  sharey=True,\n",
    "                         )\n",
    "\n",
    "\n",
    "for i in range(len(axes.flatten())): \n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sns.countplot(df, \n",
    "                        x='education', \n",
    "                        order = df['education'].value_counts().index,\n",
    "                        ax=ax)\n",
    "        ax.set_title(\"All Data\")\n",
    "        \n",
    "    else:    \n",
    "        sns.countplot(df.loc[df['merged_labels']==i-1], \n",
    "                    x='education', \n",
    "                    order = df['education'].value_counts().index,\n",
    "                    ax=ax)\n",
    "        ax.set_title(\"Cluster {}\".format(i-1))\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "plt.suptitle(\"Education Counts\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_counts = df[['merged_labels']].groupby(['merged_labels']).value_counts()\n",
    "merged_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = 1\n",
    "df_ci = df.loc[df['merged_labels']==ci, 'education']\n",
    "\n",
    "sns.barplot(100*df_ci.value_counts()/merged_counts[ci])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_ed = df.groupby([\n",
    "    \"merged_labels\", \n",
    "    \"education\",\n",
    "    ])['education'].size().unstack()\n",
    "\n",
    "df_cl_ed\n",
    "\n",
    "\n",
    "df_cl_ed.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cl_ed_pct = df_cl_ed.copy()\n",
    "for i in df['education'].unique():\n",
    "    df_cl_ed_pct[i] = 100*df_cl_ed_pct[i]/df['merged_labels'].value_counts().sort_index().values\n",
    "\n",
    "df_cl_ed_pct.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, df['merged_labels'].nunique(), \n",
    "                         figsize=(12,4),\n",
    "                         sharey=True,)\n",
    "\n",
    "for ax, clust in zip(axes.flatten(), range(df['merged_labels'].nunique())): \n",
    "    df_cl = df.loc[df['merged_labels']==clust]\n",
    "    sns.countplot(df_cl, \n",
    "                  x='education', \n",
    "                  order = df['education'].value_counts().index,\n",
    "                  ax=ax)\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    ax.set_title(\"Cluster {}\".format(clust))\n",
    "plt.suptitle(\"Count of Education by Cluster\", y=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"merged_labels\"],df[\"education\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: What other visualizations from the EDA can you think of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is step can be quite time consuming\n",
    "two_dim = TSNE(random_state=42).fit_transform(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=df['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Improve T-SNE Visualization\n",
    "\n",
    "### Exercise: Visualize using UMAP\n",
    "\n",
    "### Exercise: Visualize using Principal Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess feature importance and reclassify outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the R²\n",
    "What proportion of each variables total SS is explained between clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss_variables(df):\n",
    "    \"\"\"Get the SS for each variable\n",
    "    \"\"\"\n",
    "    ss_vars = df.var() * (df.count() - 1)\n",
    "    return ss_vars\n",
    "\n",
    "def r2_variables(df, labels):\n",
    "    \"\"\"Get the R² for each variable\n",
    "    \"\"\"\n",
    "    sst_vars = get_ss_variables(df)\n",
    "    ssw_vars = np.sum(df.groupby(labels).apply(get_ss_variables))\n",
    "    return 1 - ssw_vars/sst_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are essentially decomposing the R² into the R² for each variable\n",
    "r2_variables(df[metric_features + ['merged_labels']], 'merged_labels').drop('merged_labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Decision Tree\n",
    "We get the normalized total reduction of the criterion (gini or entropy) brought by that feature (also known as Gini importance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = df[metric_features]\n",
    "y = df.merged_labels\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fitting the decision tree\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "dt.fit(X_train, y_train)\n",
    "print(\"It is estimated that in average, we are able to predict {0:.2f}% of the customers correctly\".format(dt.score(X_test, y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing feature importance\n",
    "pd.Series(dt.feature_importances_, index=X_train.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the cluster labels of the outliers\n",
    "df_out['merged_labels'] = dt.predict(df_out[metric_features])\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizing the decision tree\n",
    "# dot_data = export_graphviz(dt, out_file=None, \n",
    "#                            feature_names=X.columns.to_list(),\n",
    "#                            filled=True,\n",
    "#                            rounded=True,\n",
    "#                            special_characters=True)\n",
    "# g = graphviz.Source(dot_data)\n",
    "\n",
    "# g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "\n",
    "# png_bytes = g.pipe(format='png')\n",
    "# with open('dt.png','wb') as f:\n",
    "#     f.write(png_bytes)\n",
    "\n",
    "# Image(png_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
